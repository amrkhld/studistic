{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"StudentPerformanceFactors.csv\")\n"
      ],
      "metadata": {
        "id": "P3VBR7SJrjAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add Student_ID to your original dataframe (df)\n",
        "\n",
        "# Check if Student_ID already exists\n",
        "if 'Student_ID' not in df.columns:\n",
        "    df['Student_ID'] = range(1, len(df) + 1)\n",
        "    print(\"âœ“ Student_ID added to original data\")\n",
        "else:\n",
        "    print(\"âœ“ Student_ID already exists\")\n",
        "\n",
        "# Show first few rows\n",
        "print(\"\\nOriginal Data with Student_ID:\")\n",
        "print(df[['Student_ID', 'Hours_Studied', 'Attendance', 'Exam_Score']].head())"
      ],
      "metadata": {
        "id": "H7Zktb-4cbJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df.columns:\n",
        "    print(col)\n"
      ],
      "metadata": {
        "id": "DLSXpVeOryRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "koBCFKK9r9M3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "yvR_ZZHJsEjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "1oszBk2TsLfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in ['Teacher_Quality', 'Parental_Education_Level' ,'Distance_from_Home'] :\n",
        "    df[col].fillna(df[col].mode()[0], inplace=True)"
      ],
      "metadata": {
        "id": "7Gfj4bBvsPVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "pXsLU2gXsUSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "C_ENJSyI3RMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "num_cols = [\n",
        "    'Hours_Studied', 'Attendance', 'Sleep_Hours',\n",
        "    'Previous_Scores', 'Tutoring_Sessions',\n",
        "    'Physical_Activity', 'Exam_Score'\n",
        "]"
      ],
      "metadata": {
        "id": "Qp8ZPotxso5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in num_cols:\n",
        "    print(f\"--- Column: {col} ---\")\n",
        "    print(df[col].describe(), \"\\n\")"
      ],
      "metadata": {
        "id": "gd9FmC7Ms0iX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df['Exam_Score'] != 101]\n",
        "\n"
      ],
      "metadata": {
        "id": "4GlggRjt54dF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for col in num_cols:\n",
        "    print(f\"--- Column: {col} ---\")\n",
        "    print(df[col].describe(), \"\\n\")\n",
        "    print(\"Top 10 values:\")\n",
        "    print(df[col].sort_values(ascending=False).head(10).values)\n",
        "    print(\"Lowest 10 values:\")\n",
        "    print(df[col].sort_values().head(10).values)\n",
        "    print(f\"Unique values: {df[col].nunique()}\")\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - 1.5 * IQR\n",
        "    upper = Q3 + 1.5 * IQR\n",
        "    outliers = df[(df[col] < lower) | (df[col] > upper)][col]\n",
        "    print(f\"Number of outliers (IQR method): {len(outliers)}\\n\")\n",
        "\n",
        "    print(\"############################\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "FoOlGJ9zs741"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cat_cols = [\n",
        "    'Parental_Involvement',\n",
        "    'Access_to_Resources',\n",
        "    'Extracurricular_Activities',\n",
        "    'Motivation_Level',\n",
        "    'Internet_Access',\n",
        "    'Family_Income',\n",
        "    'Teacher_Quality',\n",
        "    'School_Type',\n",
        "    'Peer_Influence',\n",
        "    'Learning_Disabilities',\n",
        "    'Parental_Education_Level',\n",
        "    'Distance_from_Home',\n",
        "    'Gender'\n",
        "]\n"
      ],
      "metadata": {
        "id": "UfZ_VTLltE28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in cat_cols:\n",
        "    print(f\"--- Column: {col} ---\")\n",
        "    unique_vals = df[col].unique()\n",
        "    print(f\"Unique values ({len(unique_vals)}): {unique_vals}\")\n",
        "    counts = df[col].value_counts()\n",
        "    print(\"Value counts:\\n\", counts)\n",
        "    missing = df[col].isnull().sum()\n",
        "    print(f\"Missing values: {missing}\")\n",
        "    print(\"############################\\n\")\n"
      ],
      "metadata": {
        "id": "exQOnbJYtPDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_cols = [\n",
        "    'Hours_Studied', 'Attendance', 'Sleep_Hours',\n",
        "    'Previous_Scores', 'Tutoring_Sessions',\n",
        "    'Physical_Activity', 'Exam_Score'\n",
        "]\n"
      ],
      "metadata": {
        "id": "oLDLRlXXtVnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ordinal_mapping = {\n",
        "    'Low': 1,\n",
        "    'Medium': 2,\n",
        "    'High': 3\n",
        "}\n",
        "\n",
        "ordinal_cols = [\n",
        "    'Parental_Involvement', 'Access_to_Resources',\n",
        "    'Motivation_Level', 'Family_Income', 'Teacher_Quality'\n",
        "]\n",
        "\n",
        "for col in ordinal_cols:\n",
        "    df[col] = df[col].map(ordinal_mapping)\n",
        "\n"
      ],
      "metadata": {
        "id": "3DRLsHLftjR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "binary_cols = [\n",
        "    'Extracurricular_Activities',\n",
        "    'Internet_Access',\n",
        "    'Learning_Disabilities'\n",
        "]\n",
        "\n",
        "binary_mapping = {'Yes': 1, 'No': 0}\n",
        "\n",
        "for col in binary_cols:\n",
        "    df[col] = df[col].map(binary_mapping)"
      ],
      "metadata": {
        "id": "bj5iUGLPt4LD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nominal_cols = [\n",
        "    'School_Type',\n",
        "    'Peer_Influence',\n",
        "    'Parental_Education_Level',\n",
        "    'Distance_from_Home',\n",
        "    'Gender'\n",
        "]\n",
        "df = pd.get_dummies(df, columns=nominal_cols, drop_first=True)\n"
      ],
      "metadata": {
        "id": "s3AYJu-Bt6P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separate features and target"
      ],
      "metadata": {
        "id": "M1uJb3whBeU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y = df['Exam_Score']\n",
        "X = df.drop('Exam_Score', axis=1)\n",
        "\n",
        "# Split into train (80%) and test (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Quick check\n",
        "print(f\"Training samples: {X_train.shape[0]}\")\n",
        "print(f\"Test samples: {X_test.shape[0]}\")\n",
        "print(f\"Number of features: {X_train.shape[1]}\")\n"
      ],
      "metadata": {
        "id": "mQlzhjOIuCaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LinearRegression Model\n"
      ],
      "metadata": {
        "id": "Trj78bvFs3CU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# ------------------------\n",
        "# 1. Train the model\n",
        "# ------------------------\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# ------------------------\n",
        "# 2. Predictions\n",
        "# ------------------------\n",
        "y_pred_train = lr_model.predict(X_train)\n",
        "y_pred_test = lr_model.predict(X_test)\n",
        "\n",
        "# ------------------------\n",
        "# 3. Calculate Metrics\n",
        "# ------------------------\n",
        "train_r2 = r2_score(y_train, y_pred_train)\n",
        "test_r2 = r2_score(y_test, y_pred_test)\n",
        "\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
        "\n",
        "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
        "\n",
        "# ------------------------\n",
        "# 4. Print Metrics\n",
        "# ------------------------\n",
        "print(\"=\" * 50)\n",
        "print(\"LINEAR REGRESSION RESULTS\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Training RÂ²: {train_r2:.4f}\")\n",
        "print(f\"Test RÂ²: {test_r2:.4f}\")\n",
        "print(f\"Training RMSE: {train_rmse:.2f}\")\n",
        "print(f\"Test RMSE: {test_rmse:.2f}\")\n",
        "print(f\"Test MAE: {test_mae:.2f}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# ------------------------\n",
        "# 5. Visualization: Actual vs Predicted\n",
        "# ------------------------\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test, y_pred_test, alpha=0.7, c='dodgerblue', edgecolors='w', s=80, label='Predicted vs Actual')\n",
        "plt.plot([y_test.min(), y_test.max()],\n",
        "         [y_test.min(), y_test.max()],\n",
        "         'r--', lw=3, label='Ideal Fit')\n",
        "plt.xlabel('Actual Exam Score', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Predicted Exam Score', fontsize=14, fontweight='bold')\n",
        "plt.title('Linear Regression: Actual vs Predicted', fontsize=16, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.4)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ------------------------\n",
        "# 6. Residuals Plot\n",
        "# ------------------------\n",
        "residuals = y_test - y_pred_test\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_pred_test, residuals, alpha=0.7, c='orange', edgecolors='w', s=80)\n",
        "plt.axhline(y=0, linestyle='--', color='red', lw=3)\n",
        "plt.xlabel('Predicted Exam Score', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Residuals', fontsize=14, fontweight='bold')\n",
        "plt.title('Residuals Plot', fontsize=16, fontweight='bold')\n",
        "plt.grid(alpha=0.4)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "P5fl5OmoHH_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Linear Regression model achieved a Test RÂ² score of 0.825, which means it can explain about 82% of the variance in exam scores.\n",
        "The Test RMSE was 1.52, indicating that the average prediction error is around 1.5 marks.\n",
        "Additionally, the Test MAE was only 0.41, which shows that the model predictions are very close to the actual values.\n",
        "Since the test performance is better than the training performance, the model is not overfitting and generalizes well to unseen data."
      ],
      "metadata": {
        "id": "l5pzZRRjN6oI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Model\n"
      ],
      "metadata": {
        "id": "8aYSGme6tMCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create and train Random Forest\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=15,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_train_rf = rf_model.predict(X_train)\n",
        "y_pred_test_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Metrics\n",
        "train_r2_rf = r2_score(y_train, y_pred_train_rf)\n",
        "test_r2_rf = r2_score(y_test, y_pred_test_rf)\n",
        "\n",
        "train_rmse_rf = np.sqrt(mean_squared_error(y_train, y_pred_train_rf))\n",
        "test_rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_test_rf))\n",
        "\n",
        "test_mae_rf = mean_absolute_error(y_test, y_pred_test_rf)\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"RANDOM FOREST RESULTS\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Training RÂ²: {train_r2_rf:.4f}\")\n",
        "print(f\"Test RÂ²: {test_r2_rf:.4f}\")\n",
        "print(f\"Training RMSE: {train_rmse_rf:.2f}\")\n",
        "print(f\"Test RMSE: {test_rmse_rf:.2f}\")\n",
        "print(f\"Test MAE: {test_mae_rf:.2f}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Feature Importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    \"Feature\": X_train.columns,\n",
        "    \"Importance\": rf_model.feature_importances_\n",
        "}).sort_values(\"Importance\", ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Most Important Features:\")\n",
        "print(feature_importance.head(10).to_string(index=False))\n",
        "\n",
        "# Visualization 1: Actual vs Predicted\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test, y_pred_test_rf, alpha=0.6)\n",
        "plt.plot(\n",
        "    [y_test.min(), y_test.max()],\n",
        "    [y_test.min(), y_test.max()],\n",
        "    linestyle='--',\n",
        "    linewidth=2\n",
        ")\n",
        "plt.xlabel(\"Actual Exam Score\")\n",
        "plt.ylabel(\"Predicted Exam Score\")\n",
        "plt.title(\"Random Forest: Actual vs Predicted\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualization 2: Feature Importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "top_features = feature_importance.head(10)\n",
        "plt.barh(top_features[\"Feature\"], top_features[\"Importance\"])\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"Top 10 Feature Importance\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualization 3: Comparison with Linear Regression\n",
        "# models = [\"Linear Regression\", \"Random Forest\"]\n",
        "# r2_scores = [test_r2, test_r2_rf]\n",
        "# rmse_scores = [test_rmse, test_rmse_rf]\n",
        "\n",
        "# x = np.arange(len(models))\n",
        "# width = 0.35\n",
        "\n",
        "# fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# ax1.bar(x - width/2, r2_scores, width, alpha=0.7)\n",
        "# ax1.set_ylabel(\"RÂ² Score\")\n",
        "# ax1.set_ylim(0, 1)\n",
        "\n",
        "# ax2 = ax1.twinx()\n",
        "# ax2.bar(x + width/2, rmse_scores, width, alpha=0.7)\n",
        "# ax2.set_ylabel(\"RMSE\")\n",
        "\n",
        "# ax1.set_xticks(x)\n",
        "# ax1.set_xticklabels(models)\n",
        "# ax1.set_xlabel(\"Model\")\n",
        "# ax1.set_title(\"Model Comparison: Linear Regression vs Random Forest\")\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "ZtYfP23jvsc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Network"
      ],
      "metadata": {
        "id": "FNApYLVFApDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import pandas as pd\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build ANN\n",
        "ann_model = keras.Sequential([\n",
        "    layers.Input(shape=(X_train_scaled.shape[1],)),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compile\n",
        "ann_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='mse',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "# Train\n",
        "print(\"Training Neural Network...\")\n",
        "history = ann_model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# Predictions\n",
        "y_pred_train_ann = ann_model.predict(X_train_scaled, verbose=0).flatten()\n",
        "y_pred_test_ann = ann_model.predict(X_test_scaled, verbose=0).flatten()\n",
        "\n",
        "# Metrics\n",
        "train_r2_ann = r2_score(y_train, y_pred_train_ann)\n",
        "test_r2_ann = r2_score(y_test, y_pred_test_ann)\n",
        "\n",
        "train_rmse_ann = np.sqrt(mean_squared_error(y_train, y_pred_train_ann))\n",
        "test_rmse_ann = np.sqrt(mean_squared_error(y_test, y_pred_test_ann))\n",
        "\n",
        "test_mae_ann = mean_absolute_error(y_test, y_pred_test_ann)\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"NEURAL NETWORK RESULTS\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Training RÂ²: {train_r2_ann:.4f}\")\n",
        "print(f\"Test RÂ²: {test_r2_ann:.4f}\")\n",
        "print(f\"Training RMSE: {train_rmse_ann:.2f}\")\n",
        "print(f\"Test RMSE: {test_rmse_ann:.2f}\")\n",
        "print(f\"Test MAE: {test_mae_ann:.2f}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Visualization 1: Training History\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualization 2: Actual vs Predicted\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test, y_pred_test_ann, alpha=0.6)\n",
        "plt.plot(\n",
        "    [y_test.min(), y_test.max()],\n",
        "    [y_test.min(), y_test.max()],\n",
        "    linestyle='--',\n",
        "    linewidth=2\n",
        ")\n",
        "plt.xlabel('Actual Exam Score')\n",
        "plt.ylabel('Predicted Exam Score')\n",
        "plt.title('Neural Network: Actual vs Predicted')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# =======================\n",
        "# Final Comparison\n",
        "# =======================\n",
        "\n",
        "models_all = ['Linear Regression', 'Random Forest', 'Neural Network']\n",
        "r2_all = [test_r2, test_r2_rf, test_r2_ann]\n",
        "rmse_all = [test_rmse, test_rmse_rf, test_rmse_ann]\n",
        "mae_all = [test_mae, test_mae_rf, test_mae_ann]\n",
        "\n",
        "summary_final = pd.DataFrame({\n",
        "    'Model': models_all,\n",
        "    'Test RÂ²': r2_all,\n",
        "    'Test RMSE': rmse_all,\n",
        "    'Test MAE': mae_all\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FINAL SUMMARY: ALL MODELS\")\n",
        "print(\"=\" * 60)\n",
        "print(summary_final.to_string(index=False))\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nBest Model (by RÂ²): {models_all[np.argmax(r2_all)]}\")\n",
        "print(f\"Best Model (by RMSE): {models_all[np.argmin(rmse_all)]}\")\n"
      ],
      "metadata": {
        "id": "PJLFfx85wHyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print(\"Calculating SHAP values for Linear Regression...\")\n",
        "\n",
        "# Create SHAP explainer\n",
        "explainer = shap.Explainer(lr_model, X_train)\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "print(\"âœ“ SHAP values calculated successfully!\")\n",
        "\n",
        "# Visualization 1: Feature Importance Bar Chart\n",
        "mean_shap = np.abs(shap_values.values).mean(axis=0)\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'SHAP_Importance': mean_shap\n",
        "}).sort_values('SHAP_Importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "top_10 = feature_importance_df.head(10)\n",
        "plt.barh(top_10['Feature'], top_10['SHAP_Importance'], color='skyblue', edgecolor='black')\n",
        "plt.xlabel('Mean |SHAP Value|', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Feature', fontsize=12, fontweight='bold')\n",
        "plt.title('Top 10 Features by SHAP Importance', fontsize=14, fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TOP 10 FEATURES BY SHAP IMPORTANCE\")\n",
        "print(\"=\"*60)\n",
        "print(feature_importance_df.head(10).to_string(index=False))\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Visualization 2: SHAP values for top 3 features\n",
        "top_3_features = feature_importance_df.head(3)['Feature'].values\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "for idx, feature in enumerate(top_3_features):\n",
        "    feature_idx = X_train.columns.get_loc(feature)\n",
        "    feature_values = X_test[feature].values\n",
        "    shap_vals = shap_values.values[:, feature_idx]\n",
        "\n",
        "    axes[idx].scatter(feature_values, shap_vals, alpha=0.5, edgecolors='k')\n",
        "    axes[idx].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
        "    axes[idx].set_xlabel(f'{feature} Value', fontsize=11, fontweight='bold')\n",
        "    axes[idx].set_ylabel('SHAP Value', fontsize=11, fontweight='bold')\n",
        "    axes[idx].set_title(f'Impact of {feature}', fontsize=12, fontweight='bold')\n",
        "    axes[idx].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Example Analysis: At-Risk Students\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"AT-RISK STUDENT ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "at_risk_indices = np.where(y_test < 60)[0][:3]\n",
        "\n",
        "if len(at_risk_indices) > 0:\n",
        "    for idx in at_risk_indices:\n",
        "        actual_score = y_test.iloc[idx]\n",
        "        predicted_score = lr_model.predict(X_test.iloc[[idx]])[0]\n",
        "\n",
        "        print(f\"\\nStudent #{idx}:\")\n",
        "        print(f\"  Actual Score: {actual_score:.1f}\")\n",
        "        print(f\"  Predicted Score: {predicted_score:.1f}\")\n",
        "\n",
        "        # Get SHAP contributions\n",
        "        student_shap = shap_values[idx].values\n",
        "        contributions = pd.DataFrame({\n",
        "            'Feature': X_train.columns,\n",
        "            'SHAP_Value': student_shap,\n",
        "            'Feature_Value': X_test.iloc[idx].values\n",
        "        }).sort_values('SHAP_Value', key=abs, ascending=False)\n",
        "\n",
        "        print(f\"\\n  Top Contributing Factors:\")\n",
        "        for i in range(min(5, len(contributions))):\n",
        "            row = contributions.iloc[i]\n",
        "            direction = \"â†‘\" if row['SHAP_Value'] > 0 else \"â†“\"\n",
        "            print(f\"    {direction} {row['Feature']}: {row['SHAP_Value']:+.2f} (value: {row['Feature_Value']:.2f})\")\n",
        "\n",
        "    print(\"=\"*60)\n",
        "else:\n",
        "    print(\"No at-risk students found in test set\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# Visualization 3: Example Student Breakdown\n",
        "if len(at_risk_indices) > 0:\n",
        "    student_idx = at_risk_indices[0]\n",
        "    student_shap = shap_values[student_idx].values\n",
        "\n",
        "    # Get top positive and negative contributions\n",
        "    sorted_indices = np.argsort(np.abs(student_shap))[-10:][::-1]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    colors = ['green' if val > 0 else 'red' for val in student_shap[sorted_indices]]\n",
        "\n",
        "    plt.barh(range(len(sorted_indices)), student_shap[sorted_indices], color=colors, alpha=0.7, edgecolor='black')\n",
        "    plt.yticks(range(len(sorted_indices)), X_train.columns[sorted_indices])\n",
        "    plt.xlabel('SHAP Value (Impact on Prediction)', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('Feature', fontsize=12, fontweight='bold')\n",
        "    plt.title(f'SHAP Explanation for At-Risk Student #{student_idx}\\nActual: {y_test.iloc[student_idx]:.1f}, Predicted: {lr_model.predict(X_test.iloc[[student_idx]])[0]:.1f}',\n",
        "              fontsize=13, fontweight='bold')\n",
        "    plt.axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
        "    plt.grid(axis='x', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Comparison: SHAP vs Random Forest Feature Importance\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARISON: SHAP vs Random Forest Feature Importance\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "rf_importance = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'RF_Importance': rf_model.feature_importances_\n",
        "}).sort_values('RF_Importance', ascending=False)\n",
        "\n",
        "comparison = feature_importance_df.merge(rf_importance, on='Feature')\n",
        "comparison = comparison.sort_values('SHAP_Importance', ascending=False).head(10)\n",
        "\n",
        "print(comparison.to_string(index=False))\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nâœ“ SHAP Analysis Complete!\")\n"
      ],
      "metadata": {
        "id": "k8wmN8-FxuEo",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"Building Risk Classification System...\")\n",
        "\n",
        "# Use Linear Regression predictions for the entire dataset\n",
        "df['Predicted_Score'] = lr_model.predict(df.drop('Exam_Score', axis=1))\n",
        "\n",
        "# Define Risk Levels based on Predicted Scores\n",
        "def classify_risk(score):\n",
        "    if score >= 75:\n",
        "        return \"High Performer\"\n",
        "    elif score >= 60:\n",
        "        return \"Medium Risk\"\n",
        "    else:\n",
        "        return \"At Risk\"\n",
        "\n",
        "df['Risk_Level'] = df['Predicted_Score'].apply(classify_risk)\n",
        "df['Actual_Risk_Level'] = df['Exam_Score'].apply(classify_risk)\n",
        "\n",
        "# Statistics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RISK CLASSIFICATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "risk_counts = df['Risk_Level'].value_counts()\n",
        "print(\"\\nPredicted Risk Distribution:\")\n",
        "for risk in [\"High Performer\", \"Medium Risk\", \"At Risk\"]:\n",
        "    count = risk_counts.get(risk, 0)\n",
        "    percentage = count / len(df) * 100\n",
        "    print(f\"  {risk}: {count} students ({percentage:.1f}%)\")\n",
        "\n",
        "print(\"\\nActual Risk Distribution:\")\n",
        "actual_counts = df['Actual_Risk_Level'].value_counts()\n",
        "for risk in [\"High Performer\", \"Medium Risk\", \"At Risk\"]:\n",
        "    count = actual_counts.get(risk, 0)\n",
        "    percentage = count / len(df) * 100\n",
        "    print(f\"  {risk}: {count} students ({percentage:.1f}%)\")\n",
        "\n",
        "# Accuracy\n",
        "correct = (df['Risk_Level'] == df['Actual_Risk_Level']).sum()\n",
        "accuracy = correct / len(df) * 100\n",
        "print(f\"\\nRisk Classification Accuracy: {accuracy:.2f}%\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Visualization 1: Risk Distribution Comparison\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "colors = ['red', 'orange', 'green']\n",
        "risk_order = [\"At Risk\", \"Medium Risk\", \"High Performer\"]\n",
        "\n",
        "# Predicted\n",
        "predicted_data = [risk_counts.get(risk, 0) for risk in risk_order]\n",
        "bars1 = ax1.bar(risk_order, predicted_data, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
        "ax1.set_ylabel('Number of Students', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('Predicted Risk Distribution', fontsize=14, fontweight='bold')\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "for bar, count in zip(bars1, predicted_data):\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
        "             f'{int(count)}\\n({count/len(df)*100:.1f}%)',\n",
        "             ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "# Actual\n",
        "actual_data = [actual_counts.get(risk, 0) for risk in risk_order]\n",
        "bars2 = ax2.bar(risk_order, actual_data, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
        "ax2.set_ylabel('Number of Students', fontsize=12, fontweight='bold')\n",
        "ax2.set_title('Actual Risk Distribution', fontsize=14, fontweight='bold')\n",
        "ax2.grid(axis='y', alpha=0.3)\n",
        "for bar, count in zip(bars2, actual_data):\n",
        "    height = bar.get_height()\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
        "             f'{int(count)}\\n({count/len(df)*100:.1f}%)',\n",
        "             ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualization 2: Confusion Matrix\n",
        "cm = confusion_matrix(df['Actual_Risk_Level'], df['Risk_Level'], labels=risk_order)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='RdYlGn', cbar=True,\n",
        "            xticklabels=risk_order, yticklabels=risk_order,\n",
        "            linewidths=2, linecolor='black', annot_kws={'size': 14, 'weight': 'bold'})\n",
        "plt.xlabel('Predicted Risk Level', fontsize=13, fontweight='bold')\n",
        "plt.ylabel('Actual Risk Level', fontsize=13, fontweight='bold')\n",
        "plt.title('Risk Classification Confusion Matrix', fontsize=15, fontweight='bold', pad=15)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# At-Risk Students Analysis\n",
        "at_risk = df[df['Risk_Level'] == 'At Risk']\n",
        "medium = df[df['Risk_Level'] == 'Medium Risk']\n",
        "high = df[df['Risk_Level'] == 'High Performer']\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DETAILED ANALYSIS BY RISK LEVEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for label, data in [(\"At Risk\", at_risk), (\"Medium Risk\", medium), (\"High Performer\", high)]:\n",
        "    print(f\"\\n{label}: {len(data)} students ({len(data)/len(df)*100:.1f}%)\")\n",
        "    print(f\"  Mean Exam Score: {data['Exam_Score'].mean():.2f}\")\n",
        "    print(f\"  Mean Attendance: {data['Attendance'].mean():.1f}%\")\n",
        "    print(f\"  Mean Hours Studied: {data['Hours_Studied'].mean():.1f}\")\n",
        "    print(f\"  Mean Access to Resources: {data['Access_to_Resources'].mean():.2f}/3\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Visualization 3: Feature Comparison Box Plots\n",
        "features = ['Exam_Score', 'Attendance', 'Hours_Studied', 'Access_to_Resources']\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "for idx, feature in enumerate(features):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "\n",
        "    data_list = [df[df['Risk_Level'] == risk][feature] for risk in risk_order]\n",
        "\n",
        "    bp = ax.boxplot(data_list, labels=risk_order, patch_artist=True)\n",
        "\n",
        "    for patch, color in zip(bp['boxes'], colors):\n",
        "        patch.set_facecolor(color)\n",
        "        patch.set_alpha(0.6)\n",
        "        patch.set_edgecolor('black')\n",
        "        patch.set_linewidth(2)\n",
        "\n",
        "    ax.set_ylabel(feature, fontsize=11, fontweight='bold')\n",
        "    ax.set_title(f'{feature} Distribution by Risk Level', fontsize=12, fontweight='bold')\n",
        "    ax.grid(alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Sample At-Risk Students with Recommendations\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SAMPLE AT-RISK STUDENTS - INTERVENTION RECOMMENDATIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "at_risk_sample = at_risk.head(5)\n",
        "\n",
        "for idx, row in at_risk_sample.iterrows():\n",
        "    print(f\"\\nStudent #{idx}:\")\n",
        "    print(f\"  Predicted Score: {row['Predicted_Score']:.1f}\")\n",
        "    print(f\"  Actual Score: {row['Exam_Score']:.1f}\")\n",
        "    print(f\"  Attendance: {row['Attendance']:.0f}%\")\n",
        "    print(f\"  Hours Studied: {row['Hours_Studied']:.0f}/week\")\n",
        "    print(f\"  Recommendations:\")\n",
        "\n",
        "    if row['Attendance'] < 80:\n",
        "        print(f\"    ðŸ“Œ Improve attendance (current: {row['Attendance']:.0f}%, target: 85%+)\")\n",
        "    if row['Hours_Studied'] < 20:\n",
        "        print(f\"    ðŸ“š Increase study hours (current: {row['Hours_Studied']:.0f}h, target: 25h+)\")\n",
        "    if row['Access_to_Resources'] < 2:\n",
        "        print(f\"    ðŸ’» Provide additional learning resources\")\n",
        "    if row['Parental_Involvement'] < 2:\n",
        "        print(f\"    ðŸ‘ª Encourage parental engagement\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nâœ“ Risk Classification Complete!\")\n",
        "print(f\"ðŸš¨ Total At-Risk Students: {len(at_risk)} ({len(at_risk)/len(df)*100:.1f}%)\")"
      ],
      "metadata": {
        "id": "fihsmDNGWqFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ============================================================================\n",
        "# REAL STUDENT DATA INTEGRATION\n",
        "# ============================================================================\n",
        "# Purpose: Create database with real student information (names, emails, photos)\n",
        "# for the interactive system\n",
        "# ============================================================================"
      ],
      "metadata": {
        "id": "qpl0vcKzdlT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# # Create real students data\n",
        "# # Replace with actual names, emails, and photo paths\n",
        "\n",
        "# real_students = pd.DataFrame({\n",
        "#     'Student_ID': [1, 2, 3, 4, 5],  # IDs that exist in your original data\n",
        "\n",
        "#     # PUT REAL NAMES HERE\n",
        "#     'Real_Name': [\n",
        "#         'Shimaa Mousaa',\n",
        "#         'Sara Mohamed',\n",
        "#         'Nour Ali',\n",
        "#         'Omar Khaled',\n",
        "#         'Layla Ibrahim'\n",
        "#     ],\n",
        "\n",
        "#     # PUT REAL EMAILS HERE\n",
        "#     'Real_Email': [\n",
        "#         'shimaamousaa77@gmail.com',\n",
        "#         'sara@example.com',\n",
        "#         'nour@example.com',\n",
        "#         'omar@example.com',\n",
        "#         'layla@example.com'\n",
        "#      ]\n",
        "#     #\n",
        "\n",
        "#     # # Photo paths (we'll upload photos later)\n",
        "#     # 'Photo_Path': [\n",
        "#     #     'photos/ahmed.jpg',\n",
        "#     #     'photos/sara.jpg',\n",
        "#     #     'photos/nour.jpg',\n",
        "#     #     'photos/omar.jpg',\n",
        "#     #     'photos/layla.jpg'\n",
        "#     # ]\n",
        "# })\n",
        "\n",
        "# # Show the data\n",
        "# print(\"Real Students Data:\")\n",
        "# print(real_students)\n",
        "\n",
        "# # Save to CSV\n",
        "# real_students.to_csv('real_students.csv', index=False)\n",
        "# print(\"\\nâœ“ File saved: real_students.csv\")"
      ],
      "metadata": {
        "id": "7zv_52chdn1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load real students data\n",
        "# real_students = pd.read_csv('real_students.csv')\n",
        "\n",
        "# print(\"=\"*60)\n",
        "# print(\"MERGING DATA\")\n",
        "# print(\"=\"*60)\n",
        "\n",
        "# # Merge the two dataframes\n",
        "# # This will keep ONLY students who have real data\n",
        "# merged_data = df.merge(real_students, on='Student_ID', how='inner')\n",
        "\n",
        "# print(f\"\\nOriginal data: {len(df)} students\")\n",
        "# print(f\"Real students data: {len(real_students)} students\")\n",
        "# print(f\"Merged data: {len(merged_data)} students\")\n",
        "\n",
        "# # Show sample\n",
        "# print(\"\\nSample of merged data:\")\n",
        "# print(merged_data[['Student_ID', 'Real_Name', 'Real_Email',\n",
        "#                    'Attendance', 'Hours_Studied', 'Exam_Score']].head())\n",
        "\n",
        "# # Save merged data\n",
        "# merged_data.to_csv('students_with_real_info.csv', index=False)\n",
        "# print(\"\\nâœ“ Merged data saved: students_with_real_info.csv\")\n"
      ],
      "metadata": {
        "id": "Fx5ldCKpAIcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kWDG2c831Oln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- visualization -->"
      ],
      "metadata": {
        "id": "Yy3AMP6EFKFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualization"
      ],
      "metadata": {
        "id": "FS-rYTyDFRvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "vVlrDf_QFWp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ranking of Risk Level\n",
        "risk_order = [\"At Risk\", \"Medium Risk\", \"High Performer\"]\n",
        "colors = ['red', 'orange', 'green']"
      ],
      "metadata": {
        "id": "ZdREhgbFFrzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bar plot for compare between predicted and actual risk\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14,6))\n",
        "\n",
        "pred_counts = df['Risk_Level'].value_counts()\n",
        "act_counts = df['Actual_Risk_Level'].value_counts()\n",
        "\n",
        "# Predicted\n",
        "axes[0].bar(risk_order, [pred_counts.get(r,0) for r in risk_order], color=colors, alpha=0.7, edgecolor='black')\n",
        "axes[0].set_title(\"Predicted Risk Distribution\", fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel(\"Number of Students\")\n",
        "for i, count in enumerate([pred_counts.get(r,0) for r in risk_order]):\n",
        "    axes[0].text(i, count + 5, str(count), ha='center', fontweight='bold')\n",
        "\n",
        "# Actual\n",
        "axes[1].bar(risk_order, [act_counts.get(r,0) for r in risk_order], color=colors, alpha=0.7, edgecolor='black')\n",
        "axes[1].set_title(\"Actual Risk Distribution\", fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel(\"Number of Students\")\n",
        "for i, count in enumerate([act_counts.get(r,0) for r in risk_order]):\n",
        "    axes[1].text(i, count + 5, str(count), ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2mgCE0tRF2Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(df['Actual_Risk_Level'], df['Risk_Level'], labels=risk_order)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='RdYlGn', xticklabels=risk_order, yticklabels=risk_order)\n",
        "plt.xlabel(\"Predicted Risk Level\", fontsize=12)\n",
        "plt.ylabel(\"Actual Risk Level\", fontsize=12)\n",
        "plt.title(\"Confusion Matrix of Risk Classification\", fontsize=14, fontweight='bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZLSdobuiF_YA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Boxplots for key features according risk level\n",
        "features = ['Exam_Score', 'Attendance', 'Hours_Studied', 'Previous_Scores']\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14,10))\n",
        "\n",
        "for i, feature in enumerate(features):\n",
        "    ax = axes[i//2, i%2]\n",
        "    data_list = [df[df['Risk_Level']==risk][feature] for risk in risk_order]\n",
        "    bp = ax.boxplot(data_list, patch_artist=True)\n",
        "    for patch, color in zip(bp['boxes'], colors):\n",
        "        patch.set_facecolor(color)\n",
        "        patch.set_alpha(0.6)\n",
        "        patch.set_edgecolor('black')\n",
        "        patch.set_linewidth(2)\n",
        "    ax.set_title(f\"{feature} by Risk Level\", fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel(feature)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vsc3Qlh9GPuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter plot: to the most important features\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(data=df, x='Attendance', y='Hours_Studied', hue='Risk_Level', palette=colors)\n",
        "plt.title(\"Attendance vs Hours Studied by Risk Level\", fontsize=14, fontweight='bold')\n",
        "plt.xlabel(\"Attendance (%)\")\n",
        "plt.ylabel(\"Hours Studied per week\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oN2cWgQiGTTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This scatter plot shows the relationship between studentsâ€™ Attendance and Hours Studied per week, colored by their Risk Level.\n",
        "# It helps to identify patterns such as students who study a lot but have low attendance, or students with high attendance but fewer study hours.\n",
        "# Clusters indicate different student profiles and potential academic risks."
      ],
      "metadata": {
        "id": "NIZJxz21GcAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap\n",
        "plt.figure(figsize=(12,10))\n",
        "sns.heatmap(df.select_dtypes(include='number').corr(), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=1)\n",
        "plt.title(\"Correlation Heatmap\", fontsize=16, fontweight='bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LTFOKGqGGfWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This heatmap shows the correlations between numeric student features like Hours_Studied, Attendance, Motivation_Level, Previous_Scores, and Sleep_Hours.\n",
        "# Dark red or dark blue cells highlight strong positive or negative relationships.\n",
        "# For example, higher Attendance and Hours_Studied tend to correlate with better Exam_Score, while low Sleep_Hours might negatively relate to performance.\n",
        "# This visualization helps us understand which factors most influence student outcomes and can guide the student clustering and risk-level analysis"
      ],
      "metadata": {
        "id": "XNEZCiPgGprd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Importance Plot\n",
        "# it presents the most important factors affecting the outcome.\n",
        "\n",
        "importances = rf_model.feature_importances_\n",
        "features = X.columns\n",
        "\n",
        "imp_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
        "imp_df = imp_df.sort_values(by='Importance', ascending=False).head(10)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.barh(imp_df['Feature'], imp_df['Importance'])\n",
        "plt.gca().invert_yaxis()\n",
        "plt.title(\"Top 10 Feature Importance\", fontweight='bold')\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ChnJq0aLGtSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Exam Scores (Histogram)\n",
        "# shows the distribution of student grades\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.hist(df['Exam_Score'], bins=30)\n",
        "plt.title(\"Distribution of Exam Scores\", fontweight='bold')\n",
        "plt.xlabel(\"Exam Score\")\n",
        "plt.ylabel(\"Number of Students\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "myvm_q4mGv4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This plot shows how students scored on exams.\n",
        "# Most students got average scores, while fewer students scored very high or very low.\n",
        "# It helps us see overall performance and identify students who may need extra support."
      ],
      "metadata": {
        "id": "Rq8jePOdGzui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Risk Level Percentage Pie Chart\n",
        "# it shows the precentage of each risk level\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "risk_counts = df['Risk_Level'].value_counts()\n",
        "\n",
        "scaled_counts = risk_counts.copy()\n",
        "scaled_counts['At Risk'] *= 4\n",
        "scaled_counts['High Performer'] *= 4\n",
        "\n",
        "plt.figure(figsize=(7,7))\n",
        "plt.pie(\n",
        "    scaled_counts,\n",
        "    labels=risk_counts.index,\n",
        "    autopct=lambda p: f'{p*sum(risk_counts)/100:.0f}',\n",
        "    startangle=140,\n",
        "    # colors=['blue','red','green'],\n",
        "    wedgeprops={'edgecolor':'black'}\n",
        ")\n",
        "plt.title(\"Risk Level Percentage (Enhanced Small Pieces)\", fontweight='bold')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "ev-iV9uyG22Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This pie chart shows the distribution of students by risk level. Small groups,\n",
        "# like \"At Risk\" and \"High Performer,\" are scaled up to make them more visible.\n",
        "# It helps highlight all categories clearly, even the ones with few students."
      ],
      "metadata": {
        "id": "_D9B5R3qHDdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Average Score by Category (Bar Chart)\n",
        "# according Parental_Involvement or Teacher_Quality\n",
        "\n",
        "avg_scores = df.groupby('Parental_Involvement')['Exam_Score'].mean()\n",
        "\n",
        "avg_scores.plot(kind='bar', figsize=(8,6))\n",
        "plt.title(\"Average Exam Score by Parental Involvement\", fontweight='bold')\n",
        "plt.ylabel(\"Average Score\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o4ysuKvmHRBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This bar chart shows the average exam scores for students based on their level of parental involvement.\n",
        "# Higher parental involvement tends to be associated with higher exam scores,\n",
        "# indicating the positive impact of parents supporting their children's learning."
      ],
      "metadata": {
        "id": "Yw4TuaFlHURn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pairplot to the most importance variables\n",
        "# it shows the relationships between them.\n",
        "\n",
        "sns.pairplot(df[['Attendance','Hours_Studied','Previous_Scores','Exam_Score','Risk_Level']],\n",
        "             hue='Risk_Level')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fL9m3780HVVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This pairplot visualizes the relationships between key academic features: Attendance, Hours Studied, Previous Scores, and Exam Score, colored by Risk Level.\n",
        "# It helps identify patterns such as students with higher attendance and study hours generally achieving higher scores,\n",
        "# while Risk Level highlights which students might be underperforming or at risk."
      ],
      "metadata": {
        "id": "qW3snsyCHXu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# anoter features"
      ],
      "metadata": {
        "id": "Ej-BcekoHdGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature: Lifestyle Balance\n",
        "\n",
        "# normalize Hours_Studied (0-1)\n",
        "df['Hours_norm'] = df['Hours_Studied'] / df['Hours_Studied'].max()\n",
        "\n",
        "# calculate lifestyle balance\n",
        "df['Lifestyle_Balance'] = (\n",
        "    df['Sleep_Hours'] * 0.4 +\n",
        "    df['Physical_Activity'] * 0.3 +\n",
        "    df['Hours_norm'] * 10 * 0.3\n",
        ")\n",
        "\n",
        "# drop temporary column\n",
        "df.drop('Hours_norm', axis=1, inplace=True)\n"
      ],
      "metadata": {
        "id": "rZ_SwRfsHkMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.boxplot(x='Risk_Level', y='Lifestyle_Balance', data=df,\n",
        "            order=[\"At Risk\",\"Medium Risk\",\"High Performer\"])\n",
        "\n",
        "plt.title(\"Lifestyle Balance by Risk Level\", fontweight='bold')\n",
        "plt.xlabel(\"Risk Level\")\n",
        "plt.ylabel(\"Lifestyle Balance Score\")\n",
        "plt.show()\n",
        "# it explains whether the balanced student is lives are less dangerous or not.\n"
      ],
      "metadata": {
        "id": "xdml_dXrHnuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The box plot compares lifestyle balance indicators across different student groups, highlighting differences in median values,\n",
        "# variability, and potential outliers. It provides a clear summary of how lifestyle habits vary between performance or risk categories."
      ],
      "metadata": {
        "id": "J83MIzRwHpXd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}